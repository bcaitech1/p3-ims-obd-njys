{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:58.944902Z",
     "start_time": "2021-04-22T11:06:56.623974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla P40\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import *\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamters And Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.171980Z",
     "start_time": "2021-04-22T11:06:59.167952Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16   # Mini-batch size\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.446510Z",
     "start_time": "2021-04-22T11:06:59.443508Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size = 16,\n",
    "                 num_epochs = 20,\n",
    "                 learning_rate = 0.0001,\n",
    "                 seed = 21,\n",
    "                 val_every = 1,\n",
    "                 num_workers = 4,\n",
    "                 cutmix = False,\n",
    "                 half = False,\n",
    "                 train_resize = 224,\n",
    "                 test_resize = 256,\n",
    "                 encoder_name = 'senet154',\n",
    "                 encoder_weights = \"imagenet\",\n",
    "                 n_folds = 0, \n",
    "                 gkf = False, \n",
    "                 skf = False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.val_every = val_every\n",
    "        self.num_workers = num_workers\n",
    "        self.cutmix = cutmix\n",
    "        self.half = half\n",
    "        self.train_resize = train_resize\n",
    "        self.test_resize = test_resize\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder_weights = encoder_weights\n",
    "        self.n_folds = n_folds\n",
    "        self.gkf = gkf\n",
    "        self.skf = skf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam, half=False)->tuple:\n",
    "    '''\n",
    "    랜덤한 bounding box의 좌상단,우하단 좌표 반환\n",
    "\n",
    "    Args:\n",
    "        size (tuple): batch의 shape\n",
    "        lam (float): 자를 비율\n",
    "        half (bool): 절반으로 자름\n",
    "    '''\n",
    "\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    if half==False:\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W) \n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    else:\n",
    "        bbx1 = 0\n",
    "        bby1 = 0\n",
    "        bbx2 = W//2\n",
    "        bby2 = H\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(image, mask, alpha, half=False):\n",
    "    '''\n",
    "    이미지와 마스크 컷믹스\n",
    "\n",
    "    Args:\n",
    "        image (tensor): batch 이미지\n",
    "        mask (tensor): batch 마스크\n",
    "        alpha (float): Beta Distribution의 alpha 값\n",
    "    '''\n",
    "  \n",
    "    indices = torch.randperm(image.size(0)) # 배치 크기 입력\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(image.size(), lam, half)\n",
    "    new_image = image.clone()\n",
    "    new_mask = mask.clone()\n",
    "    new_image[:, :, bby1:bby2, bbx1:bbx2] = image[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    new_mask[:, bby1:bby2, bbx1:bbx2] = mask[indices, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "    return new_image, new_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                            A.Resize(224, 224),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                           A.Resize(224, 224),\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           A.Resize(256, 256),\n",
    "                           ToTensorV2()\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Backgroud','UNKNOWN','General trash','Paper','Paper pack','Metal',\n",
    "                    'Glass','Plastic','Styrofoam','Plastic bag','Battery','Clothing']\n",
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "\n",
    "class TrashDataset(Dataset):\n",
    "    \"\"\"DataFrame format\"\"\"\n",
    "    def __init__(self, dataframe, data_dir, mode = 'Train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        self.df = dataframe\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        image_id = self.df.iloc[index]['image_id']\n",
    "        path = self.df.iloc[index]['path']\n",
    "        width = self.df.iloc[index]['width']\n",
    "        height = self.df.iloc[index]['height']      \n",
    "        images = cv2.imread(path)\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if self.mode == 'Train':\n",
    "            bin = self.df.iloc[index]['bin']\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            masks = np.zeros((height,width))\n",
    "            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for ann in anns:\n",
    "                pixel_value = ann['category_id']+1\n",
    "                masks = np.maximum(self.coco.annToMask(ann)*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, bin\n",
    "        \n",
    "        if self.mode == 'Test':\n",
    "            file_name = path.split('/')[-2:]\n",
    "            file_name = \"/\".join(file_name)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, file_name\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 데이터셋에 대한 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.10s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "alldata = pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "all_dataset = TrashDataset(alldata, data_dir='/opt/ml/input/data/train_all.json', mode = 'Train', transform = train_transform)\n",
    "all_dataloader = torch.utils.data.DataLoader(\n",
    "                    all_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    pin_memory=False,\n",
    "                    drop_last=False,\n",
    "                    shuffle=True,        \n",
    "                    num_workers=4,\n",
    "                    collate_fn=collate_fn\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold를 위한 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_dataloader(df, trn_idx, val_idx,fold):\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    data_dir = '/opt/ml/input/data/train_all.json'\n",
    "    \n",
    "    # 학습, 벨리데이션 데이터프레임 생성\n",
    "    train_df = df.iloc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_df = df.iloc[val_idx,:].reset_index(drop=True)\n",
    "    \n",
    "    # 학습, 벨리데이션 데이터셋 생성\n",
    "    print(f'\\n###### Fold:{fold} - Loading Dataset ######\\n')\n",
    "    train_ds = TrashDataset(train_df, data_dir=data_dir, transform=train_transform, mode='Train')\n",
    "    valid_ds = TrashDataset(valid_df, data_dir=data_dir, transform=val_transform, mode='Train')\n",
    "    print(f'\\n###### Fold:{fold} - Loading Dataset - DONE ######\\n')\n",
    "    \n",
    "    # 학습, 벨리데이션 데이터로더 생성\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=True,        \n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test를 위한 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataloader(df):\n",
    "    data_dir = '/opt/ml/input/data/test.json'\n",
    "    test_ds = TrashDataset(df,data_dir,'Test',test_transform)\n",
    "\n",
    "    tst_dataloader = torch.utils.data.DataLoader(\n",
    "                                                test_ds, \n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=4,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=False,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )\n",
    "    \n",
    "    return tst_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepLabV3',\n",
       " 'DeepLabV3Plus',\n",
       " 'FPN',\n",
       " 'Linknet',\n",
       " 'PAN',\n",
       " 'PSPNet',\n",
       " 'Unet',\n",
       " 'UnetPlusPlus',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'base',\n",
       " 'deeplabv3',\n",
       " 'encoders',\n",
       " 'fpn',\n",
       " 'linknet',\n",
       " 'pan',\n",
       " 'pspnet',\n",
       " 'unet',\n",
       " 'unetplusplus',\n",
       " 'utils']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from pprint import pprint\n",
    "dir(smp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(encoder_name,encoder_weights,in_channels=3,classes=12):\n",
    "    model = smp.DeepLabV3Plus(\n",
    "    encoder_name=encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=encoder_weights,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=in_channels,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=classes,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = \"timm-regnetx_320\"\n",
    "encoder_weights = \"imagenet\"\n",
    "model = get_model(encoder_name,encoder_weights)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "preprocess_input = get_preprocessing_fn(encoder_name, pretrained=encoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:34.624277Z",
     "start_time": "2021-04-22T11:15:30.068347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([1, 3, 512, 512])\n",
      "output shape :  torch.Size([1, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "\n",
    "#model = FCN8s(model = model, num_classes=12)\n",
    "x = torch.randn([1, 3, 512, 512])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x).to(device)\n",
    "print(\"output shape : \", out.size())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train And Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.901226Z",
     "start_time": "2021-04-22T11:15:38.888195Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(fold, epoch, model, valid_dataloader, criterion, device):\n",
    "    print(f'\\n- FOLD:{fold} VALIDATION #{epoch} START -\\n')\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    hist = np.zeros((12, 12))\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for step, (images, masks, _) in enumerate(valid_dataloader):\n",
    "            \n",
    "            images = torch.stack(images).to(device)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long().to(device)  # (batch, channel, height, width)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks.detach().cpu().numpy(), outputs, n_class=12)\n",
    "            \n",
    "        acc, acc_cls, mIoU, fwavacc = label_accuracy_score(hist)    \n",
    "        avrg_loss = total_loss / cnt\n",
    "        print(f'VALIDATION #{epoch}  Average Loss: {avrg_loss:.4f}, mIoU: {mIoU:.4f}, acc : {acc:.4f}')\n",
    "    print(f'\\n- FOLD:{fold} VALIDATION #{epoch} DONE - TIME:{time.time()-start_time}\\n')\n",
    "    return avrg_loss, mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.201874Z",
     "start_time": "2021-04-22T11:15:38.187884Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(fold, num_epochs, model, train_dataloader, valid_dataloader, criterion, optimizer, saved_dir, val_every, device, encoder_name):\n",
    "    print(f'- Fold:{fold} Training Start- \\n')\n",
    "    start_time = time.time()\n",
    "    best_loss = 9999999\n",
    "    best_mIoU = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, (images, masks, bin) in enumerate(train_dataloader):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            #####################################\n",
    "            # 50% 확률로 CutMix\n",
    "            mix_decision = np.random.rand()\n",
    "            if mix_decision < 0.5:\n",
    "                # cutmix(data, target, alpha)\n",
    "                images, masks = cutmix(images, masks, 1.)\n",
    "            #####################################\n",
    "                  \n",
    "            # inference\n",
    "            outputs = model(images).to(device)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_dataloader), loss.item()))\n",
    "        \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss, mIoU = validation(fold, epoch + 1, model, valid_dataloader, criterion, device)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('[loss] Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir, file_name = f'fold[{fold}]_loss_best_{encoder_name}(pretrained).pt')\n",
    "            if mIoU > best_mIoU:\n",
    "                print('[mIoU] Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_mIoU = mIoU\n",
    "                save_model(model, saved_dir, file_name = f'fold[{fold}]_mIoU_best_{encoder_name}(pretrained).pt')\n",
    "    \n",
    "    print(f'\\n- Fold:{fold} Training DONE - TIME:{time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:41.634492Z",
     "start_time": "2021-04-22T11:15:41.627493Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1 \n",
    "    \n",
    "def save_model(model, saved_dir, file_name='fcn8s_best_model(pretrained).pt'):\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "\n",
    "    # 모델 자체를 저장\n",
    "    torch.save(model, saved_dir + '/'+ file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group K Fold\n",
    "- by = bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GKF(dataframe=pd.read_csv('/opt/ml/code/alldata.csv'),data_dir='/opt/ml/input/data/train_all.json',n_splits=5):\n",
    "\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "\n",
    "    encoder_name = \"senet154\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    saved_dir = '/opt/ml/code/saved/gkf'\n",
    "\n",
    "    # bin 기준 나누기\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = gkf.split(dataframe.values, y=None, groups=dataframe['bin'].values)\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # fold별 모델\n",
    "        model = get_model(encoder_name,encoder_weights)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 데이터 로더\n",
    "        train_dataloader, valid_dataloader = get_train_valid_dataloader(dataframe, trn_idx, val_idx,fold=i+1)\n",
    "\n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # 학습\n",
    "        train(fold=i+1,\n",
    "              num_epochs=1, \n",
    "              model=model, \n",
    "              train_dataloader=train_dataloader, \n",
    "              valid_dataloader=valid_dataloader, \n",
    "              criterion=criterion, \n",
    "              optimizer=optimizer, \n",
    "              saved_dir=saved_dir, \n",
    "              val_every=val_every, \n",
    "              device=device, \n",
    "              encoder_name=encoder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Fold:1 - Loading Dataset ######\n",
      "loading annotations into memory...\n",
      "Done (t=4.12s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.25s)\n",
      "creating index...\n",
      "index created!\n",
      "###### Fold:1 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:1 Training Start- \n",
      "\n",
      "Epoch [1/1], Step [25/153], Loss: 1.5079\n",
      "Epoch [1/1], Step [50/153], Loss: 0.9618\n",
      "Epoch [1/1], Step [75/153], Loss: 0.8725\n",
      "Epoch [1/1], Step [100/153], Loss: 0.6802\n",
      "Epoch [1/1], Step [125/153], Loss: 0.5313\n",
      "Epoch [1/1], Step [150/153], Loss: 0.6674\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.4773, mIoU: 0.2210, acc : 0.8817\n",
      "[loss] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "\n",
      "- Fold:1 Training DONE -\n",
      "\n",
      "###### Fold:2 - Loading Dataset ######\n",
      "loading annotations into memory...\n",
      "Done (t=3.72s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.91s)\n",
      "creating index...\n",
      "index created!\n",
      "###### Fold:2 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:2 Training Start- \n",
      "\n",
      "Epoch [1/1], Step [25/129], Loss: 1.5111\n",
      "Epoch [1/1], Step [50/129], Loss: 1.1100\n",
      "Epoch [1/1], Step [75/129], Loss: 0.9415\n",
      "Epoch [1/1], Step [100/129], Loss: 0.7038\n",
      "Epoch [1/1], Step [125/129], Loss: 0.6600\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.7670, mIoU: 0.2362, acc : 0.8290\n",
      "[loss] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "\n",
      "- Fold:2 Training DONE -\n",
      "\n",
      "###### Fold:3 - Loading Dataset ######\n",
      "loading annotations into memory...\n",
      "Done (t=4.76s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.99s)\n",
      "creating index...\n",
      "index created!\n",
      "###### Fold:3 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:3 Training Start- \n",
      "\n",
      "Epoch [1/1], Step [25/128], Loss: 1.5081\n",
      "Epoch [1/1], Step [50/128], Loss: 1.2130\n",
      "Epoch [1/1], Step [75/128], Loss: 0.8067\n",
      "Epoch [1/1], Step [100/128], Loss: 0.8038\n",
      "Epoch [1/1], Step [125/128], Loss: 0.7642\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.5750, mIoU: 0.3026, acc : 0.8727\n",
      "[loss] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "\n",
      "- Fold:3 Training DONE -\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "alldata = pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "\n",
    "GKF(alldata,data_dir,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K Fold\n",
    "- by = bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "\n",
    "def SKF(dataframe=pd.read_csv('/opt/ml/code/alldata.csv'),data_dir='/opt/ml/input/data/train_all.json',n_splits=5):\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    encoder_name = \"senet154\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    saved_dir = '/opt/ml/code/saved/skf'\n",
    "\n",
    "    # bin 기준 나누기\n",
    "    skf = StrarifiedKFold(n_splits=n_splits)\n",
    "    folds = skf.split(dataframe.values, y=None, groups=dataframe['bin'].values)\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # fold별 모델\n",
    "        model = get_model(encoder_name,encoder_weights)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 데이터 로더\n",
    "        train_dataloader, valid_dataloader = get_train_valid_dataloader(dataframe, trn_idx, val_idx,fold=i+1)\n",
    "\n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # 학습\n",
    "        train(fold=i+1,\n",
    "              num_epochs=1, \n",
    "              model=model, \n",
    "              train_dataloader=train_dataloader, \n",
    "              valid_dataloader=valid_dataloader, \n",
    "              criterion=criterion, \n",
    "              optimizer=optimizer, \n",
    "              saved_dir=saved_dir, \n",
    "              val_every=val_every, \n",
    "              device=device, \n",
    "              encoder_name=encoder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "alldata = pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "\n",
    "SKF(alldata,data_dir,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/opt/ml/code/saved/gkf/fold[1]_loss_best_senet154(pretrained)/fold[1]_loss_best_senet154(pretrained).pt'\n",
    "model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(folder_path):\n",
    "\n",
    "    from glob import glob\n",
    "    from scipy import stats\n",
    "\n",
    "    model_list = glob(folder_path + '/*')\n",
    "    test = pd.read_csv('/opt/ml/code/testdata.csv')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    size = 256\n",
    "\n",
    "    n_folds = len(model_list)\n",
    "\n",
    "    soft_voting = [] # Fold,Data,12,256,256\n",
    "    hard_voting = [] # Fold,Data,256,256\n",
    "    \n",
    "\n",
    "    for fold in range(n_folds):\n",
    "\n",
    "        print(f'\\n@@@@@@@@@ FOLD {fold} INFERENCE START @@@@@@@@@ - TIME:0\\n')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        file_name_list = []\n",
    "        preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "        one_soft = [] # Data,12,256,256\n",
    "\n",
    "        print(f'\\n- FOLD {fold} DATASET LOAD START -\\n')\n",
    "        test_dataloader = get_test_dataloader(test)\n",
    "        model = torch.load(model_list[fold])\n",
    "        print(f'\\n- FOLD {fold} DATASET LOAD DONE -\\n')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step, (imgs, image_infos) in enumerate(test_dataloader):\n",
    "                \n",
    "                if (step+1)%5==0 or step+1==len(test_dataloader):\n",
    "                    print(f'STEP [{step+1}/{len(test_dataloader)}]')\n",
    "                \n",
    "                # inference (512 x 512)\n",
    "                outs = model(torch.stack(imgs).to(device)) # batch, 12, 512, 512\n",
    "\n",
    "                ######### soft voting #########\n",
    "                soft = outs.detach().cpu().numpy()\n",
    "                soft = soft.transpose(0,2,3,1)\n",
    "                \n",
    "                channel_list = []\n",
    "                for image in soft:\n",
    "                    transformed_mask = test_transform(image=image)['image']\n",
    "                    channel_list.append(transformed_mask)\n",
    "\n",
    "                # (batch, 12, 256, 256)\n",
    "                soft = torch.stack(channel_list)\n",
    "                soft = soft.numpy()\n",
    "                \n",
    "                one_soft.append(soft)\n",
    "                ################################\n",
    "\n",
    "                ######### hard voting #########\n",
    "                oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "                \n",
    "                # resize (256 x 256)\n",
    "                temp_mask = []\n",
    "                for img, mask in zip(np.stack(imgs), oms):\n",
    "                    transformed = A.Compose([A.Resize(size, size)])(image=img, mask=mask)\n",
    "                    mask = transformed['mask']\n",
    "                    temp_mask.append(mask)\n",
    "\n",
    "                oms = np.array(temp_mask)\n",
    "                \n",
    "                oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "                preds_array = np.vstack((preds_array, oms))\n",
    "\n",
    "                file_name_list.append([i for i in image_infos])\n",
    "                \n",
    "        # soft voting\n",
    "        one_soft = np.array(one_soft)\n",
    "        one_soft = np.concatenate(one_soft)\n",
    "        soft_voting.append(one_soft)\n",
    "        print(f'soft voting size: {one_soft.shape}')\n",
    "\n",
    "        # hard voting\n",
    "        hard_voting.append(preds_array)\n",
    "        print(f'hard voting size: {preds_array.shape}')\n",
    "        print(f'\\n@@@@@@@@@ FOLD {fold} INFERENCE DONE @@@@@@@@@ - TIME:{time.time()-start_time}\\n')\n",
    "        \n",
    "        # file names\n",
    "        if fold == 1:\n",
    "            file_names = [y for x in file_name_list for y in x]\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return file_names, soft_voting, hard_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@@@@@@@@@ FOLD 0 INFERENCE START @@@@@@@@@\n",
      "\n",
      "\n",
      "- FOLD 0 DATASET LOAD START -\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "- FOLD 0 DATASET LOAD DONE -\n",
      "\n",
      "STEP [5/53]\n",
      "STEP [10/53]\n",
      "STEP [15/53]\n",
      "STEP [20/53]\n",
      "STEP [25/53]\n",
      "STEP [30/53]\n",
      "STEP [35/53]\n",
      "STEP [40/53]\n",
      "STEP [45/53]\n",
      "STEP [50/53]\n",
      "STEP [53/53]\n",
      "soft voting size: (837, 12, 256, 256)\n",
      "hard voting size: (837, 65536)\n",
      "\n",
      "@@@@@@@@@ FOLD 0 INFERENCE DONE @@@@@@@@@ - TIME:571.7046937942505\n",
      "\n",
      "\n",
      "@@@@@@@@@ FOLD 1 INFERENCE START @@@@@@@@@\n",
      "\n",
      "\n",
      "- FOLD 1 DATASET LOAD START -\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "- FOLD 1 DATASET LOAD DONE -\n",
      "\n",
      "STEP [5/53]\n",
      "STEP [10/53]\n",
      "STEP [15/53]\n",
      "STEP [20/53]\n",
      "STEP [25/53]\n",
      "STEP [30/53]\n",
      "STEP [35/53]\n",
      "STEP [40/53]\n",
      "STEP [45/53]\n",
      "STEP [50/53]\n",
      "STEP [53/53]\n",
      "soft voting size: (837, 12, 256, 256)\n",
      "hard voting size: (837, 65536)\n",
      "\n",
      "@@@@@@@@@ FOLD 1 INFERENCE DONE @@@@@@@@@ - TIME:572.0553107261658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test set에 대한 prediction\n",
    "folder_path = '/opt/ml/code/saved/gkf'\n",
    "encoder_name = 'senet154'\n",
    "file_names, soft_voting, hard_voting = inference(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_process(soft_voting,hard_voting):\n",
    "    print(f'\\n---------- ENSEMBLING START ---------- TIME:0')\n",
    "    start_time = time.time()\n",
    "    soft_voting = np.array(soft_voting) # Fold,Data,12,256,256\n",
    "    soft_voting = np.sum(soft_voting, axis=0) # Data,12,256,256\n",
    "    soft_voting = np.argmax(soft_voting, axis=1) # Data,256,256\n",
    "    soft_voting = soft_voting.reshape([soft_voting.shape[0], size*size]).astype(int)# Data,256*256\n",
    "    \n",
    "    hard_voting = np.array(hard_voting) # Fold,Data,256*256\n",
    "    hard_voting = stats.mode(hard_voting)[0] # Data,256*256\n",
    "    hard_voting = np.squeeze(hard_voting)\n",
    "    print(f'\\n---------- ENSEMBLING DONE ---------- TIME:{time.time()-start_time}')\n",
    "    return soft_voting, hard_voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting, hard_voting = ensemble_process(soft_voting, hard_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837it [00:15, 54.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "for file_name, string in tqdm(zip(file_names, soft_voting)):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "submission.to_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_soft_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>batch_03/0947.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>batch_03/0968.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>batch_03/0969.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>batch_03/0992.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>batch_03/0998.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id                                   PredictionString\n",
       "0    batch_01_vt/0021.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "1    batch_01_vt/0028.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "2    batch_01_vt/0031.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "3    batch_01_vt/0032.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "4    batch_01_vt/0070.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "..                    ...                                                ...\n",
       "832     batch_03/0947.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "833     batch_03/0968.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "834     batch_03/0969.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "835     batch_03/0992.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "836     batch_03/0998.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "\n",
       "[837 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softdata = pd.read_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_soft_voting.csv\")\n",
    "softdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837it [00:15, 54.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "    \n",
    "for file_name, string in tqdm(zip(file_names, hard_voting)):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "submission.to_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_hard_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>batch_03/0947.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>batch_03/0968.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>batch_03/0969.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>batch_03/0992.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>batch_03/0998.jpg</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id                                   PredictionString\n",
       "0    batch_01_vt/0021.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "1    batch_01_vt/0028.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "2    batch_01_vt/0031.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "3    batch_01_vt/0032.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "4    batch_01_vt/0070.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "..                    ...                                                ...\n",
       "832     batch_03/0947.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "833     batch_03/0968.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "834     batch_03/0969.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "835     batch_03/0992.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "836     batch_03/0998.jpg  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n",
       "\n",
       "[837 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harddata = pd.read_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_hard_voting.csv\")\n",
    "harddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
