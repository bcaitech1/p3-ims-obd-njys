{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:58.944902Z",
     "start_time": "2021-04-22T11:06:56.623974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.8.1+cu102\n",
      "GPU 사용 가능 여부: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c37ffd12588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU 사용 가능 여부: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import *\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamters And Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.171980Z",
     "start_time": "2021-04-22T11:06:59.167952Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16   # Mini-batch size\n",
    "num_epochs = 35\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.446510Z",
     "start_time": "2021-04-22T11:06:59.443508Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size = 32,\n",
    "                 num_epochs = 20,\n",
    "                 learning_rate = 0.0001,\n",
    "                 seed = 21,\n",
    "                 val_every = 1,\n",
    "                 num_workers = 4,\n",
    "                 cutmix = False,\n",
    "                 half = False,\n",
    "                 train_resize = 224,\n",
    "                 test_resize = 256,\n",
    "                 encoder_name = 'senet154',\n",
    "                 encoder_weights = \"imagenet\",\n",
    "                 n_folds = 0, \n",
    "                 gkf = False, \n",
    "                 skf = False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.val_every = val_every\n",
    "        self.num_workers = num_workers\n",
    "        self.cutmix = cutmix\n",
    "        self.half = half\n",
    "        self.train_resize = train_resize\n",
    "        self.test_resize = test_resize\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder_weights = encoder_weights\n",
    "        self.n_folds = n_folds\n",
    "        self.gkf = gkf\n",
    "        self.skf = skf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://app.wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"P STAGE 3\",\n",
    "#            config={\n",
    "#                \"batch_size\": 32,\n",
    "#                \"learning_rate\": 0.0001,\n",
    "#                \"dataset\": \"COCO Trash\",\n",
    "#            })\n",
    "\n",
    "# # wandb.log({\n",
    "# #     'acc':,\n",
    "# #     'loss':,\n",
    "# # })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam, half=False)->tuple:\n",
    "    '''\n",
    "    랜덤한 bounding box의 좌상단,우하단 좌표 반환\n",
    "\n",
    "    Args:\n",
    "        size (tuple): batch의 shape\n",
    "        lam (float): 자를 비율\n",
    "        half (bool): 절반으로 자름\n",
    "    '''\n",
    "\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    if half==False:\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W) \n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    else:\n",
    "        bbx1 = 0\n",
    "        bby1 = 0\n",
    "        bbx2 = W//2\n",
    "        bby2 = H\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(image, mask, alpha, half=False):\n",
    "    '''\n",
    "    이미지와 마스크 컷믹스\n",
    "\n",
    "    Args:\n",
    "        image (tensor): batch 이미지\n",
    "        mask (tensor): batch 마스크\n",
    "        alpha (float): Beta Distribution의 alpha 값\n",
    "    '''\n",
    "  \n",
    "    indices = torch.randperm(image.size(0)) # 배치 크기 입력\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(image.size(), lam, half)\n",
    "    new_image = image.clone()\n",
    "    new_mask = mask.clone()\n",
    "    new_image[:, :, bby1:bby2, bbx1:bbx2] = image[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    new_mask[:, bby1:bby2, bbx1:bbx2] = mask[indices, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "    return new_image, new_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90, Rotate, CropNonEmptyMaskIfExists,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "train_transform = Compose([\n",
    "                            CropNonEmptyMaskIfExists(width = 512, height = 512),\n",
    "                            CenterCrop(400, 400),\n",
    "                            GridDistortion(),\n",
    "                            HorizontalFlip(),\n",
    "                            Resize(512, 512),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = Compose([\n",
    "                        ToTensorV2()\n",
    "                        ])\n",
    "\n",
    "test_transform = Compose([\n",
    "                           ToTensorV2()\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>num_objects</th>\n",
       "      <th>bin</th>\n",
       "      <th>UNKNOWN</th>\n",
       "      <th>General trash</th>\n",
       "      <th>Paper</th>\n",
       "      <th>Paper pack</th>\n",
       "      <th>Metal</th>\n",
       "      <th>Glass</th>\n",
       "      <th>Plastic</th>\n",
       "      <th>Styrofoam</th>\n",
       "      <th>Plastic bag</th>\n",
       "      <th>Battery</th>\n",
       "      <th>Clothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0002.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0003.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0005.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0006.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0007.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>3267</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0994.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>3268</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0995.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>3269</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0996.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>3270</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0997.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>3271</td>\n",
       "      <td>/opt/ml/input/data/batch_03/1000.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3272 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                     path  width  height  \\\n",
       "0            0  /opt/ml/input/data/batch_01_vt/0002.jpg    512     512   \n",
       "1            1  /opt/ml/input/data/batch_01_vt/0003.jpg    512     512   \n",
       "2            2  /opt/ml/input/data/batch_01_vt/0005.jpg    512     512   \n",
       "3            3  /opt/ml/input/data/batch_01_vt/0006.jpg    512     512   \n",
       "4            4  /opt/ml/input/data/batch_01_vt/0007.jpg    512     512   \n",
       "...        ...                                      ...    ...     ...   \n",
       "3267      3267     /opt/ml/input/data/batch_03/0994.jpg    512     512   \n",
       "3268      3268     /opt/ml/input/data/batch_03/0995.jpg    512     512   \n",
       "3269      3269     /opt/ml/input/data/batch_03/0996.jpg    512     512   \n",
       "3270      3270     /opt/ml/input/data/batch_03/0997.jpg    512     512   \n",
       "3271      3271     /opt/ml/input/data/batch_03/1000.jpg    512     512   \n",
       "\n",
       "      num_objects  bin  UNKNOWN  General trash  Paper  Paper pack  Metal  \\\n",
       "0              17    4        0              0      0           0      0   \n",
       "1              14    4        0              8      0           0      0   \n",
       "2               1    1        0              0      0           0      0   \n",
       "3               2    2        0              0      0           0      1   \n",
       "4               2    2        0              0      0           0      0   \n",
       "...           ...  ...      ...            ...    ...         ...    ...   \n",
       "3267            7    3        0              0      2           0      0   \n",
       "3268           14    4        0              0     12           0      1   \n",
       "3269            4    2        0              2      1           0      0   \n",
       "3270            8    3        0              0      8           0      0   \n",
       "3271            1    1        0              0      0           0      1   \n",
       "\n",
       "      Glass  Plastic  Styrofoam  Plastic bag  Battery  Clothing  \n",
       "0         4        4          0            9        0         0  \n",
       "1         0        0          0            6        0         0  \n",
       "2         0        1          0            0        0         0  \n",
       "3         0        1          0            0        0         0  \n",
       "4         0        2          0            0        0         0  \n",
       "...     ...      ...        ...          ...      ...       ...  \n",
       "3267      0        2          2            1        0         0  \n",
       "3268      0        0          0            1        0         0  \n",
       "3269      0        0          1            0        0         0  \n",
       "3270      0        0          0            0        0         0  \n",
       "3271      0        0          0            0        0         0  \n",
       "\n",
       "[3272 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0021.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0028.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0031.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0032.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/opt/ml/input/data/batch_01_vt/0070.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>832</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0947.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>833</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0968.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>834</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0969.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>835</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0992.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>836</td>\n",
       "      <td>/opt/ml/input/data/batch_03/0998.jpg</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id                                     path  width  height\n",
       "0           0  /opt/ml/input/data/batch_01_vt/0021.jpg    256     256\n",
       "1           1  /opt/ml/input/data/batch_01_vt/0028.jpg    256     256\n",
       "2           2  /opt/ml/input/data/batch_01_vt/0031.jpg    256     256\n",
       "3           3  /opt/ml/input/data/batch_01_vt/0032.jpg    256     256\n",
       "4           4  /opt/ml/input/data/batch_01_vt/0070.jpg    256     256\n",
       "..        ...                                      ...    ...     ...\n",
       "832       832     /opt/ml/input/data/batch_03/0947.jpg    256     256\n",
       "833       833     /opt/ml/input/data/batch_03/0968.jpg    256     256\n",
       "834       834     /opt/ml/input/data/batch_03/0969.jpg    256     256\n",
       "835       835     /opt/ml/input/data/batch_03/0992.jpg    256     256\n",
       "836       836     /opt/ml/input/data/batch_03/0998.jpg    256     256\n",
       "\n",
       "[837 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alldata=pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "testdata=pd.read_csv('/opt/ml/code/testdata.csv')\n",
    "display(alldata,testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Backgroud','UNKNOWN','General trash','Paper','Paper pack','Metal',\n",
    "                    'Glass','Plastic','Styrofoam','Plastic bag','Battery','Clothing']\n",
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "\n",
    "class TrashDataset(Dataset):\n",
    "    \"\"\"DataFrame format\"\"\"\n",
    "    def __init__(self, dataframe, data_dir, mode = 'Train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        self.df = dataframe\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        image_id = self.df.iloc[index]['image_id']\n",
    "        path = self.df.iloc[index]['path']\n",
    "        width = self.df.iloc[index]['width']\n",
    "        height = self.df.iloc[index]['height']      \n",
    "        images = cv2.imread(path)\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if self.mode == 'Train':\n",
    "            bin = self.df.iloc[index]['bin']\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            masks = np.zeros((height,width))\n",
    "            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for ann in anns:\n",
    "                pixel_value = ann['category_id']+1\n",
    "                masks = np.maximum(self.coco.annToMask(ann)*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, bin\n",
    "        \n",
    "        if self.mode == 'Test':\n",
    "            file_name = path.split('/')[-2:]\n",
    "            file_name = \"/\".join(file_name)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, file_name\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold를 위한 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_dataloader(df, trn_idx, val_idx,fold):\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    data_dir = '/opt/ml/input/data/train_all.json'\n",
    "    \n",
    "    # 학습, 벨리데이션 데이터프레임 생성\n",
    "    train_df = df.iloc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_df = df.iloc[val_idx,:].reset_index(drop=True)\n",
    "    \n",
    "    # 학습, 벨리데이션 데이터셋 생성\n",
    "    print(f'\\n###### Fold:{fold} - Loading Dataset ######\\n')\n",
    "    train_ds = TrashDataset(train_df, data_dir=data_dir, transform=train_transform, mode='Train')\n",
    "    valid_ds = TrashDataset(valid_df, data_dir=data_dir, transform=val_transform, mode='Train')\n",
    "    print(f'\\n###### Fold:{fold} - Loading Dataset - DONE ######\\n')\n",
    "    \n",
    "    # 학습, 벨리데이션 데이터로더 생성\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=True,        \n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test를 위한 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataloader(df):\n",
    "    data_dir = '/opt/ml/input/data/test.json'\n",
    "    test_ds = TrashDataset(df,data_dir,'Test',test_transform)\n",
    "\n",
    "    tst_dataloader = torch.utils.data.DataLoader(\n",
    "                                                test_ds, \n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=4,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=False,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )\n",
    "    \n",
    "    return tst_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepLabV3',\n",
       " 'DeepLabV3Plus',\n",
       " 'FPN',\n",
       " 'Linknet',\n",
       " 'PAN',\n",
       " 'PSPNet',\n",
       " 'Unet',\n",
       " 'UnetPlusPlus',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'base',\n",
       " 'deeplabv3',\n",
       " 'encoders',\n",
       " 'fpn',\n",
       " 'linknet',\n",
       " 'pan',\n",
       " 'pspnet',\n",
       " 'unet',\n",
       " 'unetplusplus',\n",
       " 'utils']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from pprint import pprint\n",
    "dir(smp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(encoder_name,encoder_weights,in_channels=3,classes=12):\n",
    "    model = smp.DeepLabV3Plus(\n",
    "    encoder_name=encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=encoder_weights,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=in_channels,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=classes,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = 'senet154'\n",
    "encoder_weights = \"imagenet\"\n",
    "model = get_model(encoder_name,encoder_weights)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "preprocess_input = get_preprocessing_fn(encoder_name, pretrained=encoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:34.624277Z",
     "start_time": "2021-04-22T11:15:30.068347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([1, 3, 512, 512])\n",
      "output shape :  torch.Size([1, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "\n",
    "#model = FCN8s(model = model, num_classes=12)\n",
    "x = torch.randn([1, 3, 512, 512])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x).to(device)\n",
    "print(\"output shape : \", out.size())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train And Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.901226Z",
     "start_time": "2021-04-22T11:15:38.888195Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(fold, epoch, model, valid_dataloader, criterion, device):\n",
    "    print(f'\\n- FOLD:{fold} VALIDATION #{epoch} START - TIME:0\\n')\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    hist = np.zeros((12, 12))\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for step, (images, masks, _) in enumerate(valid_dataloader):\n",
    "            \n",
    "            images = torch.stack(images).to(device)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long().to(device)  # (batch, channel, height, width)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks.detach().cpu().numpy(), outputs, n_class=12)\n",
    "            \n",
    "        acc, acc_cls, mIoU, fwavacc = label_accuracy_score(hist)    \n",
    "        avrg_loss = total_loss / cnt\n",
    "        print(f'VALIDATION #{epoch}  Average Loss: {avrg_loss:.4f}, mIoU: {mIoU:.4f}, acc : {acc:.4f}')\n",
    "    print(f'\\n- FOLD:{fold} VALIDATION #{epoch} DONE - TIME:{time.time()-start_time}\\n')\n",
    "    return avrg_loss, mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.201874Z",
     "start_time": "2021-04-22T11:15:38.187884Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(fold, num_epochs, model, train_dataloader, valid_dataloader, criterion, optimizer, saved_dir, val_every, device, encoder_name):\n",
    "    print(f'- Fold:{fold} Training Start - TIME:0\\n')\n",
    "    start_time = time.time()\n",
    "    best_loss = 9999999\n",
    "    best_mIoU = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f'- Fold:{fold} Epoch:{epoch+1} Training Start - TIME:0\\n')\n",
    "        epoch_start=time.time()\n",
    "        for step, (images, masks, bin) in enumerate(train_dataloader):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "#             #####################################\n",
    "#             # 50% 확률로 CutMix\n",
    "#             mix_decision = np.random.rand()\n",
    "#             if mix_decision < 0.5:\n",
    "#                 # cutmix(data, target, alpha)\n",
    "#                 images, masks = cutmix(images, masks, 1.)\n",
    "#             #####################################\n",
    "                  \n",
    "            # inference\n",
    "            outputs = model(images).to(device)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_dataloader), loss.item()))\n",
    "        \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss, mIoU = validation(fold, epoch + 1, model, valid_dataloader, criterion, device)\n",
    "#             if avrg_loss < best_loss:\n",
    "#                 print('[loss] Best performance at epoch: {}'.format(epoch + 1))\n",
    "#                 print('Save model in', saved_dir)\n",
    "#                 print()\n",
    "#                 best_loss = avrg_loss\n",
    "#                 save_model(model, saved_dir, file_name = f'fold[{fold}]_loss_best_{encoder_name}(pretrained).pt')\n",
    "            if mIoU > best_mIoU:\n",
    "                print('[mIoU] Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                print()\n",
    "                best_mIoU = mIoU\n",
    "                save_model(model, saved_dir, file_name = f'fold[{fold}]_mIoU_best_{encoder_name}(pretrained).pt')\n",
    "        print(f'- Fold:{fold} Epoch:{epoch+1} Training DONE - TIME:{time.time()-epoch_start}\\n')\n",
    "    print(f'\\n- Fold:{fold} Training DONE - TIME:{time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:41.634492Z",
     "start_time": "2021-04-22T11:15:41.627493Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1 \n",
    "    \n",
    "def save_model(model, saved_dir, file_name='fcn8s_best_model(pretrained).pt'):\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "\n",
    "    # 모델 자체를 저장\n",
    "    torch.save(model, saved_dir + '/'+ file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단일 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleModelTrain(df = pd.read_csv('/opt/ml/code/alldata.csv')):\n",
    "    \n",
    "    encoder_name = \"se_resnet101\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    saved_dir = '/opt/ml/code/saved/single'\n",
    "    \n",
    "    ####### validation 크기 조절 #######\n",
    "    train_size = int(len(df)*0.8) # 80% & 20%\n",
    "    indices = np.random.permutation(len(df))\n",
    "    trn_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size:]\n",
    "    ##################################\n",
    "    \n",
    "    model = get_model(encoder_name,encoder_weights)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n",
    "    \n",
    "    \n",
    "    train_dataloader, valid_dataloader = get_train_valid_dataloader(df, trn_idx, val_idx, 1)\n",
    "    \n",
    "    train(fold=1,\n",
    "          num_epochs=35, \n",
    "          model=model, \n",
    "          train_dataloader=train_dataloader, \n",
    "          valid_dataloader=valid_dataloader, \n",
    "          criterion=criterion, \n",
    "          optimizer=optimizer, \n",
    "          saved_dir=saved_dir, \n",
    "          val_every=val_every, \n",
    "          device=device, \n",
    "          encoder_name=encoder_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleModelTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group K Fold\n",
    "- by = bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GKF(dataframe=pd.read_csv('/opt/ml/code/alldata.csv'),data_dir='/opt/ml/input/data/train_all.json',n_splits=5):\n",
    "\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "\n",
    "    encoder_name = \"se_resnet101\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    saved_dir = '/opt/ml/code/saved/gkf'\n",
    "\n",
    "    # bin 기준 나누기\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = gkf.split(dataframe.values, y=None, groups=dataframe['bin'].values)\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # fold별 모델\n",
    "        model = get_model(encoder_name,encoder_weights)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 데이터 로더\n",
    "        train_dataloader, valid_dataloader = get_train_valid_dataloader(dataframe, trn_idx, val_idx,fold=i+1)\n",
    "\n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # 학습\n",
    "        train(fold=i+1,\n",
    "              num_epochs=35, \n",
    "              model=model, \n",
    "              train_dataloader=train_dataloader, \n",
    "              valid_dataloader=valid_dataloader, \n",
    "              criterion=criterion, \n",
    "              optimizer=optimizer, \n",
    "              saved_dir=saved_dir, \n",
    "              val_every=val_every, \n",
    "              device=device, \n",
    "              encoder_name=encoder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Fold:1 - Loading Dataset ######\n",
      "loading annotations into memory...\n",
      "Done (t=4.12s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.25s)\n",
      "creating index...\n",
      "index created!\n",
      "###### Fold:1 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:1 Training Start- \n",
      "\n",
      "Epoch [1/1], Step [25/153], Loss: 1.5079\n",
      "Epoch [1/1], Step [50/153], Loss: 0.9618\n",
      "Epoch [1/1], Step [75/153], Loss: 0.8725\n",
      "Epoch [1/1], Step [100/153], Loss: 0.6802\n",
      "Epoch [1/1], Step [125/153], Loss: 0.5313\n",
      "Epoch [1/1], Step [150/153], Loss: 0.6674\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.4773, mIoU: 0.2210, acc : 0.8817\n",
      "[loss] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "\n",
      "- Fold:1 Training DONE -\n",
      "\n",
      "###### Fold:2 - Loading Dataset ######\n",
      "loading annotations into memory...\n",
      "Done (t=3.72s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.91s)\n",
      "creating index...\n",
      "index created!\n",
      "###### Fold:2 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:2 Training Start- \n",
      "\n",
      "Epoch [1/1], Step [25/129], Loss: 1.5111\n",
      "Epoch [1/1], Step [50/129], Loss: 1.1100\n",
      "Epoch [1/1], Step [75/129], Loss: 0.9415\n",
      "Epoch [1/1], Step [100/129], Loss: 0.7038\n",
      "Epoch [1/1], Step [125/129], Loss: 0.6600\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.7670, mIoU: 0.2362, acc : 0.8290\n",
      "[loss] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "\n",
      "- Fold:2 Training DONE -\n",
      "\n",
      "###### Fold:3 - Loading Dataset ######\n",
      "loading annotations into memory...\n",
      "Done (t=4.76s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.99s)\n",
      "creating index...\n",
      "index created!\n",
      "###### Fold:3 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:3 Training Start- \n",
      "\n",
      "Epoch [1/1], Step [25/128], Loss: 1.5081\n",
      "Epoch [1/1], Step [50/128], Loss: 1.2130\n",
      "Epoch [1/1], Step [75/128], Loss: 0.8067\n",
      "Epoch [1/1], Step [100/128], Loss: 0.8038\n",
      "Epoch [1/1], Step [125/128], Loss: 0.7642\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.5750, mIoU: 0.3026, acc : 0.8727\n",
      "[loss] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/gkf\n",
      "\n",
      "- Fold:3 Training DONE -\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "alldata = pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "\n",
    "GKF(alldata,data_dir,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Stratified K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLSKF(dataframe=pd.read_csv('/opt/ml/code/alldata.csv'),data_dir='/opt/ml/input/data/train_all.json',n_splits=5):\n",
    "\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "    encoder_name = \"se_resnet101\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    saved_dir = '/opt/ml/code/saved/mlskf_aug'\n",
    "    multi_label = ['bin','UNKNOWN','General trash','Paper','Paper pack','Metal',\n",
    "                'Glass','Plastic','Styrofoam','Plastic bag','Battery','Clothing']\n",
    "\n",
    "    # bin & multi label 기준 나누기\n",
    "    mlskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2021)\n",
    "    folds = mlskf.split(dataframe.values, y=dataframe[multi_label].values)\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # fold별 모델\n",
    "        model = get_model(encoder_name,encoder_weights)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 데이터 로더\n",
    "        train_dataloader, valid_dataloader = get_train_valid_dataloader(dataframe, trn_idx, val_idx,fold=i+1)\n",
    "\n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # 학습\n",
    "        train(fold=i+1,\n",
    "              num_epochs=35, \n",
    "              model=model, \n",
    "              train_dataloader=train_dataloader, \n",
    "              valid_dataloader=valid_dataloader, \n",
    "              criterion=criterion, \n",
    "              optimizer=optimizer, \n",
    "              saved_dir=saved_dir, \n",
    "              val_every=val_every, \n",
    "              device=device, \n",
    "              encoder_name=encoder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Fold:1 - Loading Dataset ######\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=4.56s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.92s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "###### Fold:1 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:1 Training Start - TIME:0\n",
      "\n",
      "- Fold:1 Epoch:1 Training Start - TIME:0\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver error: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2d184ced7609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/opt/ml/code/alldata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mMLSKF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-09b8a4d9fd92>\u001b[0m in \u001b[0;36mMLSKF\u001b[0;34m(dataframe, data_dir, n_splits)\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m               encoder_name=encoder_name)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-430552c84f71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold, num_epochs, model, train_dataloader, valid_dataloader, criterion, optimizer, saved_dir, val_every, device, encoder_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# loss 계산 (cross entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/deeplabv3/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0maspp_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0maspp_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspp_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mhigh_res_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_copy_to_script_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/deeplabv3/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA driver error: unknown error"
     ]
    }
   ],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "alldata = pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "\n",
    "MLSKF(alldata,data_dir,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K Fold\n",
    "- by = bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "\n",
    "def SKF(dataframe=pd.read_csv('/opt/ml/code/alldata.csv'),data_dir='/opt/ml/input/data/train_all.json',n_splits=5):\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    encoder_name = \"se_resnet101\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    saved_dir = '/opt/ml/code/saved/skf'\n",
    "\n",
    "    # bin 기준 나누기\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    folds = skf.split(dataframe.values, y=dataframe['bin'].values)\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # fold별 모델\n",
    "        model = get_model(encoder_name,encoder_weights)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 데이터 로더\n",
    "        train_dataloader, valid_dataloader = get_train_valid_dataloader(dataframe, trn_idx, val_idx,fold=i+1)\n",
    "\n",
    "        # loss\n",
    "        nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # 학습\n",
    "        train(fold=i+1,\n",
    "              num_epochs=35, \n",
    "              model=model, \n",
    "              train_dataloader=train_dataloader, \n",
    "              valid_dataloader=valid_dataloader, \n",
    "              criterion=criterion, \n",
    "              optimizer=optimizer, \n",
    "              saved_dir=saved_dir, \n",
    "              val_every=val_every, \n",
    "              device=device, \n",
    "              encoder_name=encoder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Fold:1 - Loading Dataset ######\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=4.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.31s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "###### Fold:1 - Loading Dataset - DONE ######\n",
      "\n",
      "- Fold:1 Training Start - TIME:0\n",
      "\n",
      "- Fold:1 Epoch:1 Training Start - TIME:0\n",
      "\n",
      "Epoch [1/35], Step [25/82], Loss: 2.0177\n",
      "Epoch [1/35], Step [50/82], Loss: 1.3610\n",
      "Epoch [1/35], Step [75/82], Loss: 1.1005\n",
      "\n",
      "- FOLD:1 VALIDATION #1 START - TIME:0\n",
      "\n",
      "VALIDATION #1  Average Loss: 1.2184, mIoU: 0.1162, acc : 0.7443\n",
      "\n",
      "- FOLD:1 VALIDATION #1 DONE - TIME:91.01894307136536\n",
      "\n",
      "[mIoU] Best performance at epoch: 1\n",
      "Save model in /opt/ml/code/saved/skf\n",
      "\n",
      "- Fold:1 Epoch:1 Training DONE - TIME:526.5399651527405\n",
      "\n",
      "- Fold:1 Epoch:2 Training Start - TIME:0\n",
      "\n",
      "Epoch [2/35], Step [25/82], Loss: 0.8249\n",
      "Epoch [2/35], Step [50/82], Loss: 0.7231\n",
      "Epoch [2/35], Step [75/82], Loss: 0.7439\n",
      "\n",
      "- FOLD:1 VALIDATION #2 START - TIME:0\n",
      "\n",
      "VALIDATION #2  Average Loss: 0.8870, mIoU: 0.1380, acc : 0.7766\n",
      "\n",
      "- FOLD:1 VALIDATION #2 DONE - TIME:94.59400272369385\n",
      "\n",
      "[mIoU] Best performance at epoch: 2\n",
      "Save model in /opt/ml/code/saved/skf\n",
      "\n",
      "- Fold:1 Epoch:2 Training DONE - TIME:529.2732241153717\n",
      "\n",
      "- Fold:1 Epoch:3 Training Start - TIME:0\n",
      "\n",
      "Epoch [3/35], Step [25/82], Loss: 0.6952\n",
      "Epoch [3/35], Step [50/82], Loss: 0.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdcfb182f80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3d005e6f036f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/opt/ml/code/alldata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mSKF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-6438f25e04ae>\u001b[0m in \u001b[0;36mSKF\u001b[0;34m(dataframe, data_dir, n_splits)\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               encoder_name=encoder_name)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-430552c84f71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold, num_epochs, model, train_dataloader, valid_dataloader, criterion, optimizer, saved_dir, val_every, device, encoder_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# loss 계산 (cross entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/encoders/senet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pretrainedmodels/models/senet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pretrainedmodels/models/senet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mmodule_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dir = '/opt/ml/input/data/train_all.json'\n",
    "alldata = pd.read_csv('/opt/ml/code/alldata.csv')\n",
    "\n",
    "SKF(alldata,data_dir,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(folder_path):\n",
    "\n",
    "    from glob import glob\n",
    "    from scipy import stats\n",
    "\n",
    "    model_list = glob(folder_path + '/*')\n",
    "    test = pd.read_csv('/opt/ml/code/testdata.csv')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    size = 256\n",
    "\n",
    "    n_folds = len(model_list)\n",
    "\n",
    "    soft_voting = [] # Fold,Data,12,256,256\n",
    "    hard_voting = [] # Fold,Data,256,256\n",
    "    \n",
    "\n",
    "    for fold in range(n_folds):\n",
    "\n",
    "        print(f'\\n@@@@@@@@@ FOLD {fold+1} INFERENCE START @@@@@@@@@ - TIME:0\\n')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        file_name_list = []\n",
    "        preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "        one_soft = [] # Data,12,256,256\n",
    "\n",
    "        print(f'\\n- FOLD {fold+1} DATASET LOAD START -\\n')\n",
    "        test_dataloader = get_test_dataloader(test)\n",
    "        model = torch.load(model_list[fold])\n",
    "        model.eval()\n",
    "        print(f'\\n- FOLD {fold+1} DATASET LOAD DONE -\\n')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step, (imgs, image_infos) in enumerate(test_dataloader):\n",
    "                \n",
    "                if (step+1)%5==0 or step+1==len(test_dataloader):\n",
    "                    print(f'STEP [{step+1}/{len(test_dataloader)}]')\n",
    "                \n",
    "                # inference (512 x 512)\n",
    "                outs = model(torch.stack(imgs).to(device)) # batch, 12, 512, 512\n",
    "\n",
    "                ######### soft voting #########\n",
    "                soft = outs.detach().cpu().numpy()\n",
    "                soft = soft.transpose(0,2,3,1)\n",
    "                \n",
    "                channel_list = []\n",
    "                for image in soft:\n",
    "                    transformed_mask = test_transform(image=image)['image']\n",
    "                    channel_list.append(transformed_mask)\n",
    "\n",
    "                # (batch, 12, 256, 256)\n",
    "                soft = torch.stack(channel_list)\n",
    "                soft = soft.numpy()\n",
    "                \n",
    "                one_soft.append(soft)\n",
    "                ################################\n",
    "\n",
    "                ######### hard voting #########\n",
    "                oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "                \n",
    "                # resize (256 x 256)\n",
    "                temp_mask = []\n",
    "                for img, mask in zip(np.stack(imgs), oms):\n",
    "                    transformed = A.Compose([A.Resize(size, size)])(image=img, mask=mask)\n",
    "                    mask = transformed['mask']\n",
    "                    temp_mask.append(mask)\n",
    "\n",
    "                oms = np.array(temp_mask)\n",
    "                \n",
    "                oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "                preds_array = np.vstack((preds_array, oms))\n",
    "\n",
    "                file_name_list.append([i for i in image_infos])\n",
    "                \n",
    "        # soft voting\n",
    "        one_soft = np.array(one_soft)\n",
    "        one_soft = np.concatenate(one_soft)\n",
    "        soft_voting.append(one_soft)\n",
    "        print(f'soft voting size: {one_soft.shape}')\n",
    "\n",
    "        # hard voting\n",
    "        hard_voting.append(preds_array)\n",
    "        print(f'hard voting size: {preds_array.shape}')\n",
    "        print(f'\\n@@@@@@@@@ FOLD {fold+1} INFERENCE DONE @@@@@@@@@ - TIME:{time.time()-start_time}\\n')\n",
    "        \n",
    "        # file names\n",
    "        if fold == 1:\n",
    "            file_names = [y for x in file_name_list for y in x]\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return file_names, soft_voting, hard_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set에 대한 prediction\n",
    "folder_path = '/opt/ml/code/saved/mlskf_aug'\n",
    "encoder_name = \"se_resnet101\"\n",
    "file_names, soft_voting, hard_voting = inference(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_process(soft_voting,hard_voting,size):\n",
    "    print(f'\\n---------- SOFT VOTING START ---------- TIME:0\\n')\n",
    "    start_time = time.time()\n",
    "    soft_voting = np.concatenate(soft_voting,axis=0,dtype=np.float32) # Fold,Data,12,256,256\n",
    "    print(soft_voting.shape)\n",
    "    soft_voting = np.sum(soft_voting, axis=0) # Data,12,256,256\n",
    "    soft_voting = np.argmax(soft_voting, axis=1) # Data,256,256\n",
    "    soft_voting = soft_voting.reshape([soft_voting.shape[0], size*size]).astype(int)# Data,256*256\n",
    "    print(f'\\n---------- SOFT VOTING DONE ---------- TIME:{time.time()-start_time}\\n')\n",
    "    print(f'\\n---------- HARD VOTING START ---------- TIME:0\\n')\n",
    "    start_time = time.time()\n",
    "    hard_voting = np.array(hard_voting) # Fold,Data,256*256\n",
    "    hard_voting = stats.mode(hard_voting)[0] # Data,256*256\n",
    "    hard_voting = np.squeeze(hard_voting)\n",
    "    print(f'\\n---------- HARD VOTING DONE ---------- TIME:{time.time()-start_time}')\n",
    "    return soft_voting, hard_voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "soft_voting, hard_voting = ensemble_process(soft_voting, hard_voting, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "for file_name, string in tqdm(zip(file_names, soft_voting)):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "submission.to_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_soft_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softdata = pd.read_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_soft_voting.csv\")\n",
    "softdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "    \n",
    "for file_name, string in tqdm(zip(file_names, hard_voting)):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "submission.to_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_hard_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harddata = pd.read_csv(f\"/opt/ml/code/submission/{encoder_name}(pretrained)_hard_voting.csv\")\n",
    "harddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
